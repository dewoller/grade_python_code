"""
Focused integration tests for the student marking system.

This module provides essential integration tests using simple mocks
to verify the core workflow logic without complex fixture dependencies.
"""

import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
import json
import csv

# Test if we can import core modules
try:
    from src.marking.assignment_marker import AssignmentMarker
    ASSIGNMENT_MARKER_AVAILABLE = True
except ImportError:
    ASSIGNMENT_MARKER_AVAILABLE = False


class TestCoreIntegration:
    """Core integration tests using simple fixtures."""

    def test_assignment_marker_initialization(self):
        """Test that AssignmentMarker can be initialized."""
        if not ASSIGNMENT_MARKER_AVAILABLE:
            pytest.skip("AssignmentMarker not available")
            
        temp_dir = tempfile.mkdtemp()
        try:
            marker = AssignmentMarker(
                model_name="gpt-4",
                output_dir=temp_dir,
                verbosity=1
            )
            
            assert marker.model_name == "gpt-4"
            assert str(marker.output_dir) == temp_dir
            assert marker.max_retries == 3
        finally:
            shutil.rmtree(temp_dir)

    def test_perfect_student_workflow_mock(self):
        """Test complete workflow with perfect student using mocks."""
        if not ASSIGNMENT_MARKER_AVAILABLE:
            pytest.skip("AssignmentMarker not available")
            
        temp_dir = tempfile.mkdtemp()
        try:
            # Create sample files
            notebook_path, rubric_path = self._create_sample_files(temp_dir)
            
            marker = AssignmentMarker(
                model_name="gpt-4",
                output_dir=temp_dir,
                verbosity=1
            )
            
            # Mock the criterion evaluator
            with patch.object(marker, 'criterion_evaluator') as mock_evaluator:
                mock_result = MagicMock()
                mock_result.score = 3
                mock_result.confidence = 0.95
                mock_result.raw_response = "Perfect implementation"
                mock_result.error = None
                mock_result.retry_count = 0
                
                mock_evaluator.evaluate_criterion.return_value = mock_result
                
                # Execute test
                result = marker.mark_assignment("test_student", notebook_path, rubric_path)
                
                # Verify results
                assert result.student_id == "test_student"
                assert result.status in ["Completed", "Completed with Issues"]
                assert result.total_score >= 0
                assert result.max_points > 0
                
        finally:
            shutil.rmtree(temp_dir)

    def test_missing_tasks_scenario(self):
        """Test workflow with missing tasks using pure mocks."""
        mock_marker = Mock()
        mock_marker.model_name = "gpt-4"
        
        # Create mock result for missing tasks
        mock_result = Mock()
        mock_result.student_id = "missing_tasks_student"
        mock_result.total_score = 35
        mock_result.max_points = 83
        mock_result.issues = ["Task 4: MISSING_TASK", "Task 6: MISSING_TASK"]
        mock_result.status = "Completed with Issues"
        
        mock_marker.mark_assignment.return_value = mock_result
        
        # Test the mock
        result = mock_marker.mark_assignment("missing_tasks_student", "fake_path", "fake_rubric")
        
        assert result.total_score < result.max_points
        assert len(result.issues) > 0
        assert any("MISSING_TASK" in issue for issue in result.issues)

    def test_syntax_errors_scenario(self):
        """Test workflow with syntax errors using mocks."""
        mock_marker = Mock()
        
        mock_result = Mock()
        mock_result.student_id = "syntax_errors_student"  
        mock_result.total_score = 45
        mock_result.max_points = 83
        mock_result.issues = [
            "Task 2 Criterion 1: PARSING_ERROR",
            "Task 3 Criterion 2: SYNTAX_ERROR"
        ]
        mock_result.status = "Completed with Issues"
        
        mock_marker.mark_assignment.return_value = mock_result
        
        result = mock_marker.mark_assignment("syntax_errors_student", "fake_path", "fake_rubric")
        
        assert result.total_score < result.max_points
        assert any("PARSING_ERROR" in issue or "SYNTAX_ERROR" in issue for issue in result.issues)

    def test_empty_notebook_scenario(self):
        """Test workflow with empty notebook using mocks."""
        mock_marker = Mock()
        
        mock_result = Mock()
        mock_result.student_id = "empty_notebook_student"
        mock_result.total_score = 0
        mock_result.max_points = 83
        mock_result.issues = [f"Task {i}: MISSING_TASK" for i in [2, 3, 4, 5, 6, 7]]
        mock_result.status = "Completed - Zero Score"
        
        mock_marker.mark_assignment.return_value = mock_result
        
        result = mock_marker.mark_assignment("empty_notebook_student", "fake_path", "fake_rubric")
        
        assert result.total_score == 0
        assert len(result.issues) == 6

    def test_batch_processing_simulation(self):
        """Test batch processing simulation using mocks."""
        mock_marker = Mock()
        
        students = ["student_001", "student_002", "student_003"]
        mock_responses = [
            Mock(student_id="student_001", total_score=83, max_points=83, status="Completed", issues=[]),
            Mock(student_id="student_002", total_score=65, max_points=83, status="Completed with Issues", issues=["Task 4: MISSING_TASK"]),
            Mock(student_id="student_003", total_score=45, max_points=83, status="Completed with Issues", issues=["Multiple syntax errors"])
        ]
        
        mock_marker.mark_assignment.side_effect = mock_responses
        
        results = {}
        for i, student in enumerate(students):
            try:
                result = mock_marker.mark_assignment(student, f"notebooks/{student}.ipynb", "rubric.csv")
                results[student] = result
            except Exception as e:
                results[student] = Mock(student_id=student, status=f"Failed - {str(e)}", issues=[str(e)])
        
        assert len(results) == 3
        assert all(result.student_id == student_id for student_id, result in results.items())

    def test_error_recovery_workflow(self):
        """Test error recovery using mocks."""
        mock_marker = Mock()
        
        # Mock partial failure scenario
        mock_result = Mock()
        mock_result.student_id = "partial_failure_student"
        mock_result.total_score = 50
        mock_result.max_points = 83
        mock_result.issues = [
            "Task 3 Criterion 1: PARSING_ERROR",
            "Manual review required for 1 items"
        ]
        mock_result.status = "Completed with Issues"
        
        mock_marker.mark_assignment.return_value = mock_result
        
        result = mock_marker.mark_assignment("partial_failure_student", "fake_path", "fake_rubric")
        
        assert result.status == "Completed with Issues"
        assert len(result.issues) > 0
        assert any("PARSING_ERROR" in issue for issue in result.issues)

    def test_special_characters_handling(self):
        """Test workflow with special characters using mocks."""
        mock_marker = Mock()
        
        special_student_id = "Ã©tudiant_æµ‹è¯•_ðŸŽ“"
        
        mock_result = Mock()
        mock_result.student_id = special_student_id
        mock_result.total_score = 75
        mock_result.max_points = 83
        mock_result.issues = ["SpÃ©cial chars handled correctly âœ“"]
        mock_result.status = "Completed"
        
        mock_marker.mark_assignment.return_value = mock_result
        
        result = mock_marker.mark_assignment(special_student_id, "fake_path", "fake_rubric")
        
        assert result.student_id == special_student_id
        assert result.total_score > 0

    def test_excel_output_generation(self):
        """Test Excel output generation using mocks."""
        mock_excel_gen = Mock()
        mock_excel_gen.generate_marking_sheet.return_value = True
        
        mock_excel_gen.generate_marking_sheet(
            student_id="test_student",
            rubric_data={2: [{"criterion": "Basic implementation", "max_points": 2}]},
            marking_results={2: [{"score": 2, "criterion": "Basic implementation"}]},
            issues=[]
        )
        
        mock_excel_gen.generate_marking_sheet.assert_called_once()

    def test_workflow_statistics(self):
        """Test workflow statistics tracking using mocks.""" 
        if not ASSIGNMENT_MARKER_AVAILABLE:
            pytest.skip("AssignmentMarker not available")
            
        temp_dir = tempfile.mkdtemp()
        try:
            marker = AssignmentMarker(
                model_name="gpt-4",
                output_dir=temp_dir,
                verbosity=0
            )
            
            # Test initial statistics
            stats = marker.get_statistics()
            assert stats['assignments_processed'] == 0
            assert stats['total_api_calls'] == 0
            assert stats['total_errors'] == 0
            
        finally:
            shutil.rmtree(temp_dir)

    def _create_sample_files(self, temp_dir):
        """Create sample notebook and rubric files for testing."""
        # Create sample notebook
        notebook_path = Path(temp_dir) / "sample_notebook.ipynb"
        sample_notebook = {
            "cells": [
                {
                    "cell_type": "markdown",
                    "metadata": {},
                    "source": ["#### Your Solution\n"]
                },
                {
                    "cell_type": "code", 
                    "metadata": {},
                    "source": ["print('Hello World')"],
                    "execution_count": 1,
                    "outputs": []
                }
            ],
            "metadata": {
                "kernelspec": {
                    "display_name": "Python 3",
                    "language": "python", 
                    "name": "python3"
                }
            },
            "nbformat": 4,
            "nbformat_minor": 2
        }
        
        with open(notebook_path, 'w') as f:
            json.dump(sample_notebook, f)
        
        # Create sample rubric
        rubric_path = Path(temp_dir) / "sample_rubric.csv"
        rubric_data = [
            ["Task Name", "Criterion Description", "Score", "Max Points"],
            ["Task 2", "Basic implementation", "", "2"],
            ["Task 2", "Code quality", "", "1"],
            ["Task 3", "Functionality", "", "3"],
        ]
        
        with open(rubric_path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(rubric_data)
        
        return str(notebook_path), str(rubric_path)


class TestFixtureGeneration:
    """Test the fixture generation system."""
    
    def test_fixture_coordinator_mock(self):
        """Test fixture coordinator using mocks."""
        # Mock the fixture coordinator
        mock_coordinator = Mock()
        mock_coordinator.generate_all_fixtures.return_value = True
        
        # Test that it can be called
        result = mock_coordinator.generate_all_fixtures()
        assert result is True
        mock_coordinator.generate_all_fixtures.assert_called_once()

    def test_notebook_generator_mock(self):
        """Test notebook generator using mocks."""
        mock_generator = Mock()
        mock_generator.generate_all_notebooks.return_value = True
        
        result = mock_generator.generate_all_notebooks()
        assert result is True

    def test_rubric_generator_mock(self):
        """Test rubric generator using mocks."""
        mock_generator = Mock()
        mock_generator.generate_all_rubrics.return_value = True
        
        result = mock_generator.generate_all_rubrics()
        assert result is True

    def test_api_mock_generator_mock(self):
        """Test API mock generator using mocks."""
        mock_generator = Mock()
        mock_generator.generate_all_responses.return_value = True
        
        result = mock_generator.generate_all_responses()
        assert result is True

    def test_excel_mock_generator_mock(self):
        """Test Excel mock generator using mocks."""
        mock_generator = Mock()
        mock_generator.generate_all_excel_fixtures.return_value = True
        
        result = mock_generator.generate_all_excel_fixtures()
        assert result is True


if __name__ == "__main__":
    # Print import status for debugging
    print(f"AssignmentMarker available: {ASSIGNMENT_MARKER_AVAILABLE}")
    
    # Run a simple test
    test = TestCoreIntegration()
    test.test_missing_tasks_scenario()
    print("Basic mock test passed!")
